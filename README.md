# Dissecting RainbowÂ DQN in QuantitativeÂ Trading

[![MadeÂ withÂ PyTorch](https://img.shields.io/badge/PyTorch-2.1%2B-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org) [![License:Â MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE) [![All Contributors](https://img.shields.io/badge/all_contributors-3-orange.svg?style=flat-square)](#contributors)

> **Modular RainbowÂ DQN reâ€‘implementation inÂ PyTorch with an easyâ€toâ€‘toggle component switchboard, benchmarked on three Atari games (*Seaquest*, *Asterix*, *RoadÂ Runner*) and a highâ€‘volatility Forex trading simulator.**

---
## TableÂ ofÂ Contents
1. [Overview](#overview)
2. [Key Features](#key-features)
3. [DirectoryÂ Structure](#directory-structure)
4. [Installation](#installation)
5. [QuickÂ Start](#quick-start)
   * [AtariÂ experiments](#atari-experiments)
   * [ForexÂ experiments](#forex-experiments)
   * [Hyperâ€‘parameterÂ sweeps](#hyper-parameter-sweeps)
6. [Configuration](#configuration)
7. [ReproducingÂ theÂ PaperÂ Results](#reproducing-the-paper-results)
8. [LoggingÂ &Â Checkpoints](#logging--checkpoints)
9. [BuildingÂ theÂ Report](#building-the-report)
10. [Related Papers](#related-papers)
11. [Contributors](#contributors)
12. [License](#license)
13. [Authors](#authors)

---
## Overview
RainbowÂ DQN \[HesselÂ etÂ al.,Â 2018\] merges six powerful extensions to the original DQN algorithm.  Our project:

* **Reâ€‘implements Rainbow from scratch** in a **single, unified codebase** (`CombinedAgent`, `CombinedNetwork`, `CombinedBuffer`).
* **Toggles every Rainbow component** (`--useDouble`, `--usePrioritized`, `--useDuel`, `--useNoisy`, `--useDistributive`, `--useNstep`) from the command lineÂ â€“ perfect for ablation studies.
* Benchmarks performance on **classic RL control (AtariÂ 2600)** *and* on a **realistic financial trading task** (gymâ€‘anytrading Forex).
* Ships with **hyperâ€‘parameter sweep utilities**, full training logs, and LaTeX sources for the accompanying report.

---
## KeyÂ Features
|Â ComponentÂ |Â FlagÂ |Â ReferenceÂ |
|-----------|------|-----------|
|Â DoubleÂ DQNÂ |Â `--useDouble`Â | VanÂ HasseltÂ etÂ al.,Â 2016 |
|Â Prioritized Experience ReplayÂ |Â `--usePrioritized`Â | SchaulÂ etÂ al.,Â 2015 |
|Â Dueling Network ArchitectureÂ |Â `--useDuel`Â | WangÂ etÂ al.,Â 2016 |
|Â Noisy NetworksÂ |Â `--useNoisy`Â | FortunatoÂ etÂ al.,Â 2017 |
|Â Nâ€‘step ReturnsÂ |Â `--useNstep`Â | SuttonÂ &Â Barto,Â 2018 |
|Â Categorical Distributional RL (C51)Â |Â `--useDistributive`Â | BellemareÂ etÂ al.,Â 2017 |

Combined via the **`CombinedAgent`** class, each component can be *enabled or disabled independently* for clean, reproducible ablation.

---
## DirectoryÂ Structure
```text
.
â”œâ”€â”€ data/                     # data utilities (e.g. currency conversion)
â”‚Â Â  â””â”€â”€ convert_fx.py
â”œâ”€â”€ Legacy/                  # early, standalone agents kept for reference
â”‚Â Â  â”œâ”€â”€ util/                #   â””â”€ Buffer/Network variants
â”‚Â Â  â””â”€â”€ ...                  #   (ddqn.py, dqn.py, ...)
â”œâ”€â”€ util/
â”‚Â Â  â”œâ”€â”€ CombinedBuffer.py
â”‚Â Â  â”œâ”€â”€ CombinedNetwork.py
â”‚Â Â  â”œâ”€â”€ SegmentTree.py
â”‚Â Â  â””â”€â”€ running_mean.py
â”œâ”€â”€ combined_agent.py         # â­ core training logic
â”œâ”€â”€ main.py                   # â­ commandâ€‘line entryâ€‘point
â”œâ”€â”€ script.py                 # hyperâ€‘parameter grid search helper
â”œâ”€â”€ params.py                 # default CLI / training parameters
â”œâ”€â”€ plot.py                   # postâ€‘training visualisation
â”œâ”€â”€ report/                   # LaTeX sources (includes `main.tex` shown above)
â””â”€â”€ test_checkpoints/         # savedÂ `.npy` reward curves (autoâ€‘created)
```

> **Tip:**Â Every experimental flag in *`params.py`* can be overridden at runtime; see [Configuration](#configuration).

---
## Installation
> Tested on **PythonÂ 3.10+** with *PyTorchÂ 2.1*, *gymnasiumÂ 0.29*, and *gymâ€‘anytradingÂ 2.0*.

```bash
# 1.Â Create an isolated environment
conda create -n rainbow-trading python=3.10
conda activate rainbow-trading

# 2.Â Install Python dependencies
pip install torch gymnasium[atari] gym-anytrading numpy matplotlib tqdm

# (Optional)Â If you need Atari ROMs via AutoROM:
pip install autorom[accept-rom-license]
autorom --accept-license
```

---
## QuickÂ Start
### AtariÂ experiments
Run full Rainbow on **Seaquest** for 700 episodes Ã—Â 700 steps:
```bash
python main.py \
  -env SeaquestNoFrameskip-v4 \
  --num_episodes 700 --max_steps 700 \
  --memory_size 500000 --batch_size 32 \
  --target_update_freq 8000 --epsilon_decay_steps 490000 \
  --lr 1e-4 --omega 0.6 --beta 0.4 --gamma 0.99 \
  --sigma_init 0.5 --n_step 3 --atom_size 51 \
  --useDouble --usePrioritized --useDuel \
  --useNoisy --useDistributive --useNstep
```
(The same command works for **AsterixNoFrameskip-v4** and **RoadRunnerNoFrameskip-v4** â€“ just change `-env`.)

### ForexÂ experiments
```bash
python main.py \
  -env forex-v0 \                    # custom gymâ€‘anytrading env
  --num_episodes 900 --max_steps 700 \
  --batch_size 256 --memory_size 80000 \
  --useDouble --usePrioritized --useDuel \
  --useDistributive --useNstep        # Noisy disabled (paper insight)
```

### Hyperâ€‘parameterÂ sweeps
`script.py` launches an **8â€‘way parallel grid search** (edit `num_processes` as needed):
```bash
python script.py            # runs >30 experiment combinations
```
Results land inÂ `results/`.

---
## Configuration
All defaults live in **`params.py`**.Â Every field is exposed as a CLI flag, e.g.:
```bash
# halve the target update period and bump hidden size
python main.py -env AsterixNoFrameskip-v4 -target_update_freq 4000 -hidden_dim 1024
```

Key parameters:
|Â VariableÂ |Â DefaultÂ |Â MeaningÂ |
|----------|---------|---------|
|Â `NUMBER_STEPS`Â |Â 700Â | max env steps per episode |
|Â `NUM_TOTAL_EPISODES`Â |Â 900Â | training episodes (Atari) |
|Â `MEMORY_SIZE`Â |Â 500â€¯000Â | replay buffer capacity |
|Â `LEARNING_RATE`Â |Â 1â€¯Ã—â€¯10â»â´Â | Adam/RMSpropÂ `lr` |
|Â `SIGMA_INIT`Â |Â 0.5Â | initial stdâ€‘dev for Noisy layers |

---
## ReproducingÂ theÂ PaperÂ Results
|Â ExperimentÂ |Â CommandÂ |Â Expected wallâ€‘clockÂ |
|------------|---------|---------------------|
| **Full Rainbow â€“ Seaquest** | see *Atari* quickâ€‘start above |Â ~90Â min on RTXâ€‘3070 |
| **Ablation (No Noisy)** | add `--useNoisy`Â âš ï¸ï¸ _omitted_ | â€“6â€¯% training time |
| **Forex baseline DQN** | omit all `--use*` flags | ~25Â min on AppleÂ M2 |

Preâ€‘trained reward curves (`.npy`) live inÂ `test_checkpoints/` and can be plotted via:
```bash
python plot.py --input test_checkpoints/FullRainbow.npy
```

---
## LoggingÂ &Â Checkpoints
* **Rewards** â€“ NumPy arrays saved automatically to `test_checkpoints/<config>.npy`.
* **Distributional plots** â€“ if C51 is enabled, PNG heatâ€‘maps appear under `distribution_plots/`.
* (Planned) **TensorBoard** â€“ run `tensorboard --logdir runs/` once PRÂ #12 merges.

---
## BuildingÂ theÂ Report
LaTeX sources live in `report/` (or the root if copied there).

```bash
cd report/
latexmk -pdf main.tex   # rebuilds Dissecting_Rainbow_DQN.pdf
```

---
## Citation
If you use this codebase, please cite:
```bibtex
@misc{lafond2025rainbow,
  title        = {Dissecting Rainbow DQN in Quantitative Trading},
  author       = {Fong, Max and Lafond, William K. and Tsariov, Denis},
  year         = {2025},
  note         = {\url{https://github.com/<yourâ€‘repoâ€‘url>}},
}
```
---
## Related Papers

01. [V. Mnih et al., "Human-level control through deep reinforcement learning." Nature, 518 (7540):529â€“533, 2015.](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
02. [H. van Hasselt et al., "Deep Reinforcement Learning with Double Q-learning." arXiv:1509.06461, 2015.](https://arxiv.org/pdf/1509.06461.pdf)
03. [T. Schaul et al., "Prioritized Experience Replay." arXiv:1511.05952, 2015.](https://arxiv.org/pdf/1511.05952.pdf)
04. [Z. Wang et al., "Dueling Network Architectures for Deep Reinforcement Learning." arXiv:1511.06581, 2015.](https://arxiv.org/pdf/1511.06581.pdf)
05. [M. Fortunato et al., "Noisy Networks for Exploration." arXiv:1706.10295, 2017.](https://arxiv.org/pdf/1706.10295.pdf)
06. [M. G. Bellemare et al., "A Distributional Perspective on Reinforcement Learning." arXiv:1707.06887, 2017.](https://arxiv.org/pdf/1707.06887.pdf)
07. [R. S. Sutton, "Learning to predict by the methods of temporal differences." Machine Learning, 3(1):9â€“44, 1988.](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf)
08. [M. Hessel et al., "Rainbow: Combining Improvements in Deep Reinforcement Learning." arXiv:1710.02298, 2017.](https://arxiv.org/pdf/1710.02298.pdf)

---
## Contributors

Thanks goes to these wonderful people ;)

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="20%">
        <a href="https://github.com/max-fong">
          <img src="https://avatars.githubusercontent.com/u/143747815?v=4" width="100px;" alt="Max Fong"/><br />
          <sub><b>Max Fong</b></sub>
        </a><br />ğŸ’» ğŸ“–
      </td>
      <td align="center" valign="top" width="20%">
        <a href="https://github.com/William-Lafond">
          <img src="https://avatars.githubusercontent.com/u/98282992?v=4" width="100px;" alt="William Kiem Lafond"/><br />
          <sub><b>William Kiem Lafond</b></sub>
        </a><br />ğŸ’» ğŸ“– ğŸ“Š
      </td>
      <td align="center" valign="top" width="20%">
        <a href="https://github.com/denistsariov">
          <img src="https://avatars.githubusercontent.com/u/107961778?v=4" width="100px;" alt="Denis Tsariov"/><br />
          <sub><b>Denis Tsariov</b></sub>
        </a><br />ğŸ’» ğŸ§ª
      </td>
    </tr>
  </tbody>
</table>
<!-- markdownlint-enable -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

---
## License
This repository is released under the **MIT License**.  See [`LICENSE`](LICENSE) for details.

---
## Authors
* **MaxÂ Fong** â€“ categorical module, codebase refactor, scripts.
* **William KiemÂ Lafond** â€“ Prioritized, DuelNet, NoisyNet components, report writing.
* **DenisÂ Tsariov** â€“ base DQN/DDQN agents, N-step Learning, hyperâ€‘parameter tuning.

Project for *COMPÂ 579 DeepÂ ReinforcementÂ Learning* with Doina Precup and Isabeau PrÃ©mont-Schwarz, McGillÂ University,Â 2025.

